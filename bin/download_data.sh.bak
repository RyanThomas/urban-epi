
# This bash script downloads all data for the Urban EPI from the source, as well as setting up the proper directory structure.

DIR=~/projects/urban-epi/
GRASSDB=~/grassdb/


DATA=$DIR/data
TMP=$DATA/tmp/ # temporary files that get deleted after each download
# raw data for indicators go here
GLCF=$DATA/glcf/ # land cover data
POP=$DATA/population # population
CITIES=$DATA/cities # all the cities individually 


cd $DIR

# land cover data fromftp://ftp.glcf.umd.edu/glcf/Global_LNDCVR/UMD_TILES/Version_5.1/2012.01.01
# MCD12 is the code for land cover data from NASA.
mkdir -p $GLCF && mkdir $TMP
wget -r ftp://ftp.glcf.umd.edu/glcf/Global_LNDCVR/UMD_TILES/Version_5.1/2012.01.01/* -P $TMP
cp -r $TMP/ftp.glcf.umd.edu/glcf/Global_LNDCVR/UMD_TILES/Version_5.1/2012.01.01/MCD12Q1_V51_LC1.2012*/*.tif.gz $GLCF
find $GLCF -name '*.gz' -exec gunzip '{}' \;
mkdir $GLCF/tifs && mv $GLCF/*.tif $GLCF/tifs
rm -rf $TMP/*

gdalbuildvrt  -overwrite   $GLCF/landuse_cover.vrt  $GLCF/tifs/*.tif  #Land Cover


## Population density from University of Columbia's SEDAC, CEISN.
wget --user=ryan_thomas --password=Indic@tor5 http://sedac.ciesin.columbia.edu/downloads/data/gpw-v4/gpw-v4-population-count-adjusted-to-2015-unwpp-country-totals/gpw-v4-population-count-adjusted-to-2015-unwpp-country-totals-2015.zip  -P  $TMP

unzip -f $TMP/*.zip  -d $data/pop_density

############################################
# Hansen tree cover loss
# This is a very large file. To minimize the data storage and download time,
# we only download the necessary files. The download links are in 10x10 tiles,
# so the conditional here writes the appropriate URLS to a text file and downloads them.

## NOTE!! This must be done AFTER city shapefiles are downloaded.

cd $CITIES
ogr2ogr -t_srs EPSG:4326 -overwrite  seoul_rep.shp seoul*.shp
for file in $(find . -name "*.shp"); do
    printf "\nWorking on: " 
    echo $file
    #set the lat/lon variables
    lon=$(ogrinfo -al -geom=NO $file | grep Extent | awk -F '[( ,).]' '{ print  $3}')
    lat=$(ogrinfo -al -geom=NO $file | grep Extent | awk -F '[( ,).]' '{ print  $6}')
    # Get lon url value
    if  [[ $lon -gt 0 && $lon -lt 10 ]] ; then 
        url_tlon=$(echo $lon | awk '{print substr($0, 1, length($0)-1)0 "00E"}') ; 
        elif [[ $lon -gt 10 ]] ; then
        url_tlon=$(echo $lon | awk '{print substr($0, 1, length($0)-1)0 "E"}')
        else 
        url_lon=$(echo $lon | awk '{print substr($0, 1, length($0)-1)0 "W"}' | awk '{print substr($0,2)}')
    fi
    
    echo "Longitudinal value is: " $lon
    # get lat url value
    if [[ $lat -gt 0 && $lat -lt 10 ]]; then 
        url_tlat=$(echo $lat | awk '{print substr($0, 1, length($0)-1)0 "0N"}')
        elif [[ lat -gt 10 ]] ; then
        url_tlat=$(echo $lat | awk '{print substr($0, 1, length($0)-1)0 "N"}')
        else 
        url_lat=$(echo $lat | awk '{print substr($0, 1, length($0)-1)0 "S"}' | awk '{print substr($0,2)}')
    fi
    echo "Latitudinal value is: " $lat
    printf "URL for: " 
    echo $file "\n"
    echo https://storage.googleapis.com/earthenginepartners-hansen/GFC2015/Hansen_GFC2015_treecover2000_$url_lat\_$url_lon.tif
done





ogrinfo -al -geom=NO ./seoul/tl_2016_06_tract/tl_2016_06_tract.shp | grep Extent | awk -F '[( ,).]' '{ print  $3}' 



https://storage.googleapis.com/earthenginepartners-hansen/GFC2015/Hansen_GFC2015_treecover2000_00N_000E.tif

ogrinfo -al -geom=NO seoul_rep.shp | grep Extent | awk -F '[( ,).]' '{ print $6 }' | awk '{print substr($0, 1, length($0)-1)0 "E"}'
ogrinfo -al -geom=NO seoul_rep.shp | grep Extent | awk -F '[( ,).]' '{ print $6 }' | awk '{print substr($0, 1, length($0)-1)0 "W"}'
cd $DIR

cd $TMP

mkdir treecover/ && cd treecover
while read URL; do
  wget $URL
  done < $DIR/treecover/treecover_gain_loss_study_areas.txt

cd ..


#################################################################################################
# Download shapefile with urban areas into new directory.
# For now, I am waiting to include this step, bounding boxes defined manually.
#mkdir ../ne_10m_urban_areas && cd ne_10m_urban_areas
#wget http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_urban_areas.zip
#####################################################






